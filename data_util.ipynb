{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  \n",
    "import json\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "import spacy\n",
    "import json\n",
    "# from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# from pandas import Series\n",
    "import logging\n",
    "import numpy\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize pairs count data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "f = open('corpus_stats.pkl','rb')  \n",
    "corpus_stats = pickle.load(f)  \n",
    "print (type(corpus_stats) )  #show file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word2id', 'id2word', 'word_freq', 'vocab_size', 'num_pairs', 'num_sentences'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100026"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_stats[\"vocab_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "f = open('pairs_count.pkl','rb')  \n",
    "pairs_count = pickle.load(f)  \n",
    "print (type(pairs_count) )  #show file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['amod', 'nsubj', 'dobj', 'nsubj_amod', 'dobj_amod'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_count[\"nsubj_amod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14917\n"
     ]
    }
   ],
   "source": [
    "print(pairs_count[\"amod\"][(17, 19)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amod': 139, 'nsubj': 77, 'dobj': 77, 'nsubj_amod': 17, 'dobj_amod': 23}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bar_dict = dict()\n",
    "\n",
    "for key in pairs_count.keys():\n",
    "\n",
    "    data = np.array(list(pairs_count[key].values()))\n",
    "    # print(np.mean(data))\n",
    "    s = np.sort(data)\n",
    "\n",
    "    bar_dict[key] = s[-100000]\n",
    "    \n",
    "print(bar_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize knowledge graph data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install -r requirements.txt\n",
    "# # !python setup.py develop\n",
    "# from aser.database.db_API import KG_Connection\n",
    "# from aser.database.db_API import generate_event_id, generate_relation_id, generate_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_conn = KG_Connection(db_path=r'KG_wino.db', mode='cache')\n",
    "# print('SIZE:', 'eventualities: ', len(kg_conn.event_id_set), 'relations:', len(kg_conn.relation_id_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_id_list = list(kg_conn.event_id_set)\n",
    "# event_id_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_conn.get_exact_match_event(event_id_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(zip(kg_conn.event_columns, kg_conn.event_column_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize SP-10K data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\sp10k_data\\amod_annotation.txt\n",
      ".\\sp10k_data\\dobj_amod_annotation.txt\n",
      ".\\sp10k_data\\dobj_annotation.txt\n",
      ".\\sp10k_data\\nsubj_amod_annotation.txt\n",
      ".\\sp10k_data\\nsubj_annotation.txt\n",
      ".\\sp10k_data\\wino_dobj_amod_annotation.txt\n",
      ".\\sp10k_data\\wino_nsubj_amod_annotation.txt\n"
     ]
    }
   ],
   "source": [
    "path = '.\\sp10k_data'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    \n",
    "def sp_tsv2wino(filename):\n",
    "    with open(filename) as tsvfile:\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        reader = csv.reader(tsvfile, dialect='excel-tab')\n",
    "        \n",
    "        if \"nsubj\" in filename:\n",
    "            if \"amod\" in filename:\n",
    "                data_type = \"s-a\"\n",
    "            else:\n",
    "                data_type = \"s\"\n",
    "        elif \"dobj\" in filename:\n",
    "            if \"amod\" in filename:\n",
    "                data_type = \"o-a\"\n",
    "            else:\n",
    "                data_type = \"o\"\n",
    "        else:\n",
    "            data_type = \"a\"\n",
    "        \n",
    "        print(data_type)\n",
    "        \n",
    "        for row in reader:\n",
    "            if data_type == \"s-a\":\n",
    "                temp = []\n",
    "                text = \"A \"+ row[0] + \" B, \" + \"he be \" + row[1]\n",
    "                temp.append(text)\n",
    "                temp.append(\"A\")\n",
    "                data_list.append(temp)\n",
    "            elif data_type == \"o-a\":\n",
    "                temp = []\n",
    "                text = \"A \"+ row[0] + \" B, \" + \"he be \" + row[1]\n",
    "                temp.append(text)\n",
    "                temp.append(\"B\")\n",
    "                data_list.append(temp)\n",
    "        \n",
    "        return data_list\n",
    "\n",
    "def list2txt(data_list, filename):\n",
    "    file = open(filename,\"w\")\n",
    "    \n",
    "    for sample in data_list:\n",
    "        file.write(sample[0])\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"he\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"A\")\n",
    "        file.write(\",\")\n",
    "        file.write(\"B\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(sample[1])\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "    \n",
    "# data_list = sp_tsv2wino(files[1])\n",
    "# print(len(data_list))\n",
    "# list2txt(data_list, \"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the wino data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \n",
      "C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "C:\\Users\\xzhao\\Anaconda3\\envs\\Keras\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': {'txt1': 'The city councilmen refused the demonstrators a permit because', 'pron': 'they', 'txt2': 'feared violence.'}, 'answers': ['The city councilmen', 'The demonstrators'], 'correctAnswer': 'A', 'source': '(Winograd1972)'}\n",
      "{'text': {'txt1': 'The city councilmen refused the demonstrators a permit because', 'pron': 'they', 'txt2': 'feared violence.'}, 'answers': ['The city councilmen', 'The demonstrators'], 'correctAnswer': 'A', 'source': '(Winograd1972)', 'verbs': ['refuse', 'fear'], 'noun1': ['city', 'councilman'], 'noun2': ['demonstrator']}\n"
     ]
    }
   ],
   "source": [
    "# Load WSC dataset\n",
    "\n",
    "\n",
    "tree = etree.parse('WSCollection.xml')\n",
    "root = tree.getroot()\n",
    "original_problems = root.getchildren()\n",
    "problems = list()\n",
    "\n",
    "for original_problem in original_problems:\n",
    "    problem = dict()\n",
    "    for information in original_problem.getchildren():\n",
    "        \n",
    "        if information.tag == 'answers':\n",
    "            answers = information.getchildren()\n",
    "            answer_list = list()\n",
    "            for answer in answers:\n",
    "                answer_list.append(answer.text.strip())\n",
    "            problem['answers'] = answer_list\n",
    "        elif information.tag == 'text':\n",
    "            texts = information.getchildren()\n",
    "            # print(texts)\n",
    "            text_dict = dict()\n",
    "            for text1 in texts:\n",
    "                text_dict[text1.tag] = text1.text.replace('\\n', ' ').strip()\n",
    "            problem['text'] = text_dict\n",
    "        elif information.tag == 'quote':\n",
    "            pass\n",
    "        else:\n",
    "            problem[information.tag] = information.text.replace(' ', '')\n",
    "    problems.append(problem)\n",
    "\n",
    "# all_sentences = list()\n",
    "# for question in problems:\n",
    "#     sentence = question['text']['txt1'] + ' ' + question['text']['pron'] + ' ' + question['text']['txt2']\n",
    "#     all_sentences.append(sentence)\n",
    "\n",
    "print(problems[0])\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# stemmer.stem(\"running\")\n",
    "\n",
    "for question in problems:\n",
    "    sentence = question['text']['txt1'] + ' ' + question['text']['pron'] + ' ' + question['text']['txt2']\n",
    "    doc = nlp(sentence)\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    # stem_verbs = []\n",
    "    cand1_doc = nlp(question[\"answers\"][0])\n",
    "    cand2_doc = nlp(question[\"answers\"][1])\n",
    "    \n",
    "    noun1= [token.lemma_ for token in cand1_doc if (token.pos_ in [\"NOUN\",\"PROPN\"])]\n",
    "    noun2= [token.lemma_ for token in cand2_doc if (token.pos_ in [\"NOUN\",\"PROPN\"])]\n",
    "    \n",
    "    if not noun1:\n",
    "        noun1 = [question[\"answers\"][0]]\n",
    "    if not noun2:\n",
    "        noun2 = [question[\"answers\"][1]]\n",
    "        \n",
    "    question[\"verbs\"] = verbs\n",
    "    question[\"noun1\"] = noun1\n",
    "    question[\"noun2\"] = noun2\n",
    "    \n",
    "print(problems[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of the stored data\n",
    "sample = dict()\n",
    "sample[\"text\"] = \"\"\n",
    "sample[\"candA\"] = \"\"\n",
    "sample[\"candB\"] = \"\"\n",
    "sample[\"ans\"] = \"A\"\n",
    "sample[\"pron\"] = \"he\"\n",
    "sample[\"source\"] = \"\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "sample[\"format\"] = \"\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "sample[\"score\"] = 0 # importance, not yet used\n",
    "sample[\"coverage\"] = 0 # portion of the tokens in the orignal question coverged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a data collection for all the data\n",
    "data_collection = dict()\n",
    "for i in range(273):\n",
    "    data_collection[i] = []\n",
    "    \n",
    "# construct a related words sheet for all 273 queries\n",
    "related_words = dict()\n",
    "related_words[\"noun1\"] = []\n",
    "related_words[\"noun2\"] = []\n",
    "related_words[\"verbs\"] = []\n",
    "\n",
    "for i in range(273):\n",
    "    question = problems[i]\n",
    "    related_words[\"noun1\"].append(question[\"noun1\"])\n",
    "    related_words[\"noun2\"].append(question[\"noun2\"])\n",
    "    related_words[\"verbs\"].append(question[\"verbs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\sp10k_data\\amod_annotation.txt\n",
      ".\\sp10k_data\\dobj_amod_annotation.txt\n",
      ".\\sp10k_data\\dobj_annotation.txt\n",
      ".\\sp10k_data\\nsubj_amod_annotation.txt\n",
      ".\\sp10k_data\\nsubj_annotation.txt\n",
      ".\\sp10k_data\\wino_dobj_amod_annotation.txt\n",
      ".\\sp10k_data\\wino_nsubj_amod_annotation.txt\n",
      "a\n",
      "o-a\n",
      "o\n",
      "s-a\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "# let's start from sp10k\n",
    "\n",
    "path = '.\\sp10k_data'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    \n",
    "sp_data_collection = dict()\n",
    "for i in range(273):\n",
    "    sp_data_collection[i] = []\n",
    "    \n",
    "def sp_util(filename, data_collection, related_words, problems):\n",
    "    with open(filename) as tsvfile:\n",
    "        \n",
    "        reader = csv.reader(tsvfile, dialect='excel-tab')\n",
    "        \n",
    "        if \"nsubj\" in filename:\n",
    "            if \"amod\" in filename:\n",
    "                data_type = \"s-a\"\n",
    "            else:\n",
    "                data_type = \"s\"\n",
    "        elif \"dobj\" in filename:\n",
    "            if \"amod\" in filename:\n",
    "                data_type = \"o-a\"\n",
    "            else:\n",
    "                data_type = \"o\"\n",
    "        else:\n",
    "            data_type = \"a\"\n",
    "        \n",
    "        print(data_type)\n",
    "        \n",
    "        for row in reader:\n",
    "            if data_type == \"s-a\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" \" + row[0] + \" \"  + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"nsubj_amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage      \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                        \n",
    "            elif data_type == \"o-a\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" \" + row[0] + \" \" + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"dobj_amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "            elif data_type == \"s\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candB + \", he \" + row[0] + \".\"\n",
    "                        sample[\"candA\"] = row[1]\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"nsubj\" # [\"amod\",\"v\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candA + \", he \" + row[0] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = row[1]\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"nsubj\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "            elif data_type == \"o\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candB + \", \" + row[0] + \" he.\"\n",
    "                        sample[\"candA\"] = row[1]\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"dobj\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candA + \", \" + row[0] + \" he.\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = row[1]\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"dobj\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "            elif data_type == \"a\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in noun1:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" and \" + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "                    if row[0] in noun2:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" and \" + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"sp10k\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "        \n",
    "        return True\n",
    "\n",
    "for f in files:\n",
    "    if not \"wino\" in f:\n",
    "        sp_util(f, sp_data_collection, related_words, problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(273):\n",
    "#     print(len(data_collection[i]))\n",
    "\n",
    "# for data in data_collection[0]:\n",
    "#     print(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amod\n",
      "a\n",
      "nsubj\n",
      "s\n",
      "dobj\n",
      "o\n",
      "nsubj_amod\n",
      "s-a\n",
      "dobj_amod\n",
      "o-a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then pairs count\n",
    "\n",
    "def pc_util(data_collection, related_words, pairs_count,corpus_stats, bar_dict, problems):\n",
    "    \n",
    "    for raw_type in list(pairs_count.keys()):\n",
    "        print(raw_type)\n",
    "        if \"nsubj\" in raw_type:\n",
    "            if \"amod\" in raw_type:\n",
    "                data_type = \"s-a\"\n",
    "            else:\n",
    "                data_type = \"s\"\n",
    "        elif \"dobj\" in raw_type:\n",
    "            if \"amod\" in raw_type:\n",
    "                data_type = \"o-a\"\n",
    "            else:\n",
    "                data_type = \"o\"\n",
    "        else:\n",
    "            data_type = \"a\"\n",
    "\n",
    "        print(data_type)\n",
    "\n",
    "        for key, value in pairs_count[raw_type].items():\n",
    "            row = []\n",
    "            row.append(corpus_stats[\"id2word\"][key[0]])\n",
    "            row.append(corpus_stats[\"id2word\"][key[1]])\n",
    "            row.append(str(value))\n",
    "                \n",
    "            if value < bar_dict[raw_type]:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            if data_type == \"s-a\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" \" + row[0] + \" \"  + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"nsubj_amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                        \n",
    "            elif data_type == \"o-a\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" \" + row[0] + \" \" + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"dobj_amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "            elif data_type == \"s\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candB + \", he \" + row[0] + \".\"\n",
    "                        sample[\"candA\"] = row[1]\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"nsubj\" # [\"amod\",\"v\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candA + \", he \" + row[0] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = row[1]\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\"# [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"nsubj\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "            elif data_type == \"o\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candB + \", \" + row[0] + \" he.\"\n",
    "                        sample[\"candA\"] = row[1]\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"dobj\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "                    if row[0] in verbs:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = row[1] + \" and \" + candA + \", \" + row[0] + \" he.\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = row[1]\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"dobj\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "            elif data_type == \"a\":\n",
    "                for i in range(273):\n",
    "                    noun1 = related_words[\"noun1\"][i]\n",
    "                    noun2 = related_words[\"noun2\"][i]\n",
    "                    verbs = related_words[\"verbs\"][i]\n",
    "                    \n",
    "                    candA = problems[i][\"answers\"][0]\n",
    "                    candB = problems[i][\"answers\"][1]\n",
    "                    \n",
    "                    if row[0] in noun1:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" and \" + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "                    \n",
    "                    if row[0] in noun2:\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = candA + \" and \" + candB + \", he be \" + row[1] + \".\"\n",
    "                        sample[\"candA\"] = candA\n",
    "                        sample[\"candB\"] = candB\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"he\"\n",
    "                        sample[\"source\"] = \"pairs_count\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"amod\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(row[2]) # importance, not yet used\n",
    "                        \n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "                        \n",
    "                        data_collection[i].append(sample)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "pc_data_collection = dict()\n",
    "for i in range(273):\n",
    "    pc_data_collection[i] = []\n",
    "\n",
    "pc_util(pc_data_collection, related_words, pairs_count,corpus_stats, bar_dict, problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(273):\n",
    "#     print(len(pc_data_collection[i]))\n",
    "\n",
    "# for data in pc_data_collection[0]:\n",
    "#     print(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '5d5f2556c910aa10a390207f5f565d789af3563d', 'event1_id': '224308091a0c5def2f1292c84a366972e5feb666', 'event2_id': '214a330751eed3e386114ac068c96e316fc88dcc', 'Precedence': 0.0, 'Succession': 0.0, 'Synchronous': 0.0, 'Reason': 1.0, 'Result': 0.0, 'Condition': 0.0, 'Contrast': 0.0, 'Concession': 0.0, 'Conjunction': 0.0, 'Instantiation': 0.0, 'Restatement': 0.0, 'ChosenAlternative': 0.0, 'Alternative': 0.0, 'Exception': 0.0, 'Co_Occurrence': 1.0, 'event1': {'_id': '224308091a0c5def2f1292c84a366972e5feb666', 'verbs': 'place', 'skeleton_words_clean': 'place in water', 'skeleton_words': 'it place in water', 'words': 'it can not be place in the water', 'pattern': 'spass-v-X-o', 'frequency': 1.0}, 'event2': {'_id': '214a330751eed3e386114ac068c96e316fc88dcc', 'verbs': 'refuse', 'skeleton_words_clean': 'official refuse permission', 'skeleton_words': 'official refuse permission', 'words': 'united states port security official have refuse permission', 'pattern': 's-v-o', 'frequency': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "path = '.\\KG_by_question\\\\'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.json' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "# for f in files:\n",
    "#     print(f)\n",
    "    \n",
    "with open(files[0]) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally from KG\n",
    "lemma_stop_words = [\"do\",\"be\"]\n",
    "\n",
    "def kg_util(files, data_collection, related_words, problems, stop_words):\n",
    "    \n",
    "    for f in files:\n",
    "        \n",
    "        for i in range(273):\n",
    "            if str(i+1) in f:\n",
    "                rank = i\n",
    "        # print(rank)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            with open(f) as json_file:\n",
    "                data = json.load(json_file)\n",
    "                a = 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        with open(f) as json_file:\n",
    "            \n",
    "            data = json.load(json_file)\n",
    "            \n",
    "            for instance in data:\n",
    "                \n",
    "                if instance[\"Co_Occurrence\"] < 10:\n",
    "                    # print(\"my_faults\")\n",
    "                    continue\n",
    "                \n",
    "                sverbs = instance[\"event1\"][\"verbs\"]\n",
    "                sverbs += \" \"\n",
    "                sverbs += instance[\"event2\"][\"verbs\"]\n",
    "                \n",
    "                v_doc = nlp(sverbs)\n",
    "                lemma_v = [token.lemma_ for token in v_doc]\n",
    "                \n",
    "                useless = 0\n",
    "                for verb in lemma_v:\n",
    "                    if verb in stop_words:\n",
    "                        useless = 1\n",
    "                        \n",
    "                if useless:\n",
    "                    # print(\"useless\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    event1_doc = nlp(instance[\"event1\"][\"skeleton_words\"])\n",
    "                    event2_doc = nlp(instance[\"event2\"][\"skeleton_words\"])\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                obj1 = [token.lemma_ for token in event1_doc if (token.pos_ in [\"PROPN\", \"NOUN\"])]\n",
    "                obj2 = [token.lemma_ for token in event2_doc if (token.pos_ in [\"PROPN\", \"NOUN\"])]\n",
    "                    \n",
    "                pairs = []\n",
    "                unpaired = []\n",
    "                \n",
    "                for obj in obj1:\n",
    "                    if obj in obj2:\n",
    "                        pairs.append(obj)\n",
    "                    else:\n",
    "                        unpaired.append(obj)\n",
    "                \n",
    "                if not pairs:\n",
    "                    # print(\"lol\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                if len(unpaired) == 0:\n",
    "                    for obj in obj2:\n",
    "                        if obj not in pairs:\n",
    "                            unpaired.append(obj)\n",
    "                \n",
    "                if len(unpaired) == 0:\n",
    "                    unpaired.append(\"A\")\n",
    "                \n",
    "                    \n",
    "                for paired_obj in pairs:\n",
    "                    try:\n",
    "                        event1_text = instance[\"event1\"][\"skeleton_words\"]\n",
    "                        event2_text = instance[\"event2\"][\"skeleton_words\"].replace(paired_obj, \"[MASK]\")\n",
    "\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = event1_text + \", \" +  event2_text + \".\"                  \n",
    "\n",
    "                        sample[\"candA\"] = paired_obj\n",
    "                        sample[\"candB\"] = unpaired[0]\n",
    "                        sample[\"ans\"] = \"A\"\n",
    "                        sample[\"pron\"] = \"[MASK]\"\n",
    "                        sample[\"source\"] = \"aser\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"aser\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(instance[\"Co_Occurrence\"]) # importance, not yet used\n",
    "\n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage \n",
    "\n",
    "                        data_collection[rank].append(sample)\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "                for paired_obj in pairs:\n",
    "                    \n",
    "                    try:\n",
    "                        event1_text = instance[\"event1\"][\"skeleton_words\"].replace(paired_obj, \"[MASK]\")\n",
    "                        event2_text = instance[\"event2\"][\"skeleton_words\"]\n",
    "\n",
    "                        sample = dict()\n",
    "                        sample[\"text\"] = event2_text + \", \" +  event1_text + \".\"                  \n",
    "\n",
    "                        sample[\"candA\"] = unpaired[0]\n",
    "                        sample[\"candB\"] = paired_obj\n",
    "                        sample[\"ans\"] = \"B\"\n",
    "                        sample[\"pron\"] = \"[MASK]\"\n",
    "                        sample[\"source\"] = \"aser\" # [\"aser\",\"sp10k\",\"pairs_count\"]\n",
    "                        sample[\"format\"] = \"aser\" # [\"amod\",\"nsubj\",\"dobj\",\"nsubj_amod\",\"dobj_amod\",\"aser\"]\n",
    "                        sample[\"score\"] = str(instance[\"Co_Occurrence\"]) # importance, not yet used\n",
    "\n",
    "                        sentence = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "                        doc = nlp(sentence)\n",
    "                        useful_tokens = [token.lemma_ for token in doc if token.pos_ in [\"VERB\",\"NOUN\",\"PROPN\",\"ADP\",\"ADJ\",\"ADV\"]]\n",
    "                        cov = 0\n",
    "                        for token in useful_tokens:\n",
    "                            if token in sample[\"text\"]:\n",
    "                                cov+=1\n",
    "                        coverage = cov/len(useful_tokens)\n",
    "                        sample[\"coverage\"] = coverage      \n",
    "\n",
    "                        data_collection[rank].append(sample)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "kg_data_collection = dict()\n",
    "for i in range(273):\n",
    "    kg_data_collection[i] = []  \n",
    "                \n",
    "kg_util(files, kg_data_collection, related_words, problems, lemma_stop_words)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(273):\n",
    "    print(len(kg_data_collection[i]))\n",
    "\n",
    "for data in kg_data_collection[0]:\n",
    "    print(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectifier(data_collection,format_list,source_list,score_dict):\n",
    "    \n",
    "    rec_data_collecton = dict()\n",
    "    \n",
    "    for i in range(273):\n",
    "        rec_data_collecton[i] = []\n",
    "        \n",
    "        for sample in data_collection[i]:\n",
    "            if not sample[\"source\"] in source_list:\n",
    "                continue\n",
    "             \n",
    "            if not sample[\"format\"] in format_list:\n",
    "                continue\n",
    "                \n",
    "            if float(sample[\"score\"]) < float(score_dict[sample[\"source\"]][sample[\"format\"]]):\n",
    "                continue\n",
    "                \n",
    "            rec_data_collecton[i].append(sample)\n",
    "            \n",
    "    return rec_data_collecton\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2txt(data_collection, problems, filename):\n",
    "    \n",
    "    for i in range(273):\n",
    "        \n",
    "        path = \"./\"+filename+\"/\"+str(i)+\"/\"\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except:\n",
    "            a = 1\n",
    "            \n",
    "            \n",
    "        file = open(path + \"train.c.txt\",\"w\")\n",
    "\n",
    "        for sample in data_collection[i]:\n",
    "            file.write(sample[\"text\"])\n",
    "            file.write(\"\\n\")\n",
    "            file.write(sample[\"pron\"])\n",
    "            file.write(\"\\n\")\n",
    "            file.write(sample[\"candA\"])\n",
    "            file.write(\",\")\n",
    "            file.write(sample[\"candB\"])\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            if sample[\"ans\"] == \"A\":\n",
    "                label = sample[\"candA\"]\n",
    "            else:\n",
    "                label = sample[\"candB\"]\n",
    "                \n",
    "            file.write(label)\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "        file.close()\n",
    "        \n",
    "        file2 = open(path + \"test.c.txt\",\"w\")\n",
    "        \n",
    "        text = problems[i]['text']['txt1'] + ' ' + problems[i]['text']['pron'] + ' ' + problems[i]['text']['txt2']\n",
    "        \n",
    "        file2.write(text)\n",
    "        file2.write(\"\\n\")\n",
    "\n",
    "        file2.write(problems[i]['text']['pron'])\n",
    "        file2.write(\"\\n\")\n",
    "\n",
    "        file2.write(problems[i][\"answers\"][0])\n",
    "        file2.write(\",\")\n",
    "        file2.write(problems[i][\"answers\"][1])\n",
    "        file2.write(\"\\n\") \n",
    "\n",
    "        file2.write(problems[i][\"correctAnswer\"])\n",
    "        file2.write(\"\\n\") \n",
    "        file2.write(\"\\n\") \n",
    "            \n",
    "data2txt(sp_data_collection, problems, \"sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
